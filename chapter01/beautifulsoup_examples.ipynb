{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クローリング&スクレイピング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoupでスクレイピング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental usage of BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html形式で記述されている対象から特定の部分を抽出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = スクレイピングとは\n",
      "p1 = Webページを解析すること。\n",
      "p2 = 任意の箇所を抽出すること。\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <h1>スクレイピングとは</h1>\n",
    "    <p>Webページを解析すること。</p>\n",
    "    <p>任意の箇所を抽出すること。</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "#Make BeautifulSoup Instance(1st: Object of Analysis, Format of analysis objects)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "print(\"h1 = \" + h1.string)\n",
    "print(\"p1 = \" + p1.string)\n",
    "print(\"p2 = \" + p2.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のIDで要素を検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title = スクレイピングとは\n",
      "#body = Webページから任意のデータを抽出すること。\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <h1 id = \"title\">スクレイピングとは</h1>\n",
    "    <p id = \"body\">Webページから任意のデータを抽出すること。</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "title = soup.find(id = \"title\")\n",
    "body = soup.find(id = \"body\")\n",
    "\n",
    "print(\"#title = \"  + title.string)\n",
    "print(\"#body = \" + body.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数要素の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uta > http://uta.pw\n",
      "oto > http://oto.chu.jp\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <ul>\n",
    "       <li><a href = \"http://uta.pw\">uta</a></li>\n",
    "       <li><a href = \"http://oto.chu.jp\">oto</a></li>\n",
    "   </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "#Make BeautifulSoup Instance\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#get all \"a\" tags in html source code.\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    text = a.string\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.find_all()`メソッドの戻り値は次のような形の配列となっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://uta.pw\">uta</a>, <a href=\"http://oto.chu.jp\">oto</a>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " BeautifulSoupインスタンス作成時に使用するhtmlをwebから取得:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京都 渋谷区 宇田川町\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://api.aoikujira.com/zip/xml/1500042\"\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "ken = soup.find(\"ken\").string\n",
    "shi = soup.find(\"shi\").string\n",
    "cho = soup.find(\"cho\").string\n",
    "print(ken, shi, cho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSSセレクタを使用\n",
    "\n",
    "BeautifulSoupでは、javascriptのライブラリjQueryのように、CSSセレクタを使用して任意の要素を抽出することが可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 =  トルストイの名言\n",
      "li =  復習するは我にあり、我は仇をかえさん\n",
      "li =  光あるうち光の中を歩め\n",
      "li =  強い人々は、いつも気取らない\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "<div id = \"meigen\">\n",
    "    <h1>トルストイの名言</h1>\n",
    "    <ul class = \"items\">\n",
    "        <li>復習するは我にあり、我は仇をかえさん</li>\n",
    "        <li>光あるうち光の中を歩め</li>\n",
    "        <li>強い人々は、いつも気取らない</li>\n",
    "    </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#get title\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string  #CSSセレクタで要素を一つ取り出す\n",
    "print(\"h1 = \", h1)\n",
    "\n",
    "#get contents\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list:\n",
    "    print(\"li = \", li.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yahoo!ファイナンスの為替情報を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/jpy= 108.260000\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://stocks.finance.yahoo.co.jp/stocks/detail/?code=usdjpy\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "price = soup.select_one(\".stoksPrice\").string\n",
    "print(\"usd/jpy=\", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://stocks.finance.yahoo.co.jp/stocks/detail/?code=usdjpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aタグ: アンカーの略。リンクの出発点と到達点を表す\n",
    "\n",
    "`href`:Hypertext referenceの略称　リンク先の指定に利用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
